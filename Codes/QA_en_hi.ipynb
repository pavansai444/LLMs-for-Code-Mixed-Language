{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09b074e-030a-480b-891b-23cda06f4f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/qa_en_hi'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537fbd51-358d-4e5b-9610-e5fce97082b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  LBP.zip\n",
      "  inflating: LBP/qa.py               \n",
      "   creating: LBP/QA_EN_HI/\n",
      " extracting: LBP/QA_EN_HI/cached_dev_bert-base-multilingual-cased_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_dev_bert-base-multilingual-uncased_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_dev_bert-fa-QA-v1_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_dev_bert-large-uncased-whole-word-masking-finetuned-squad_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_dev_BiomedVLP-CXR-BERT-general_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_dev_distilbert-base-uncased_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_dev_muril-base-cased_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_dev_xlm-clm-ende-1024_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_dev_xlm-mlm-en-2048_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_dev_xlm-mlm-ende-1024_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_dev_xlm-roberta-base_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_train_bert-base-en-hi-codemix-cased_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_train_bert-base-multilingual-cased_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_train_bert-base-multilingual-uncased_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_train_bert-base-multilingual-uncased_720.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_train_bert-fa-QA-v1_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_train_bert-large-uncased-whole-word-masking-finetuned-squad_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_train_BiomedVLP-CXR-BERT-general_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_train_distilbert-base-uncased_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_train_muril-base-cased_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_train_xlm-clm-ende-1024_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_train_xlm-mlm-en-2048_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_train_xlm-mlm-ende-1024_512.zip  \n",
      " extracting: LBP/QA_EN_HI/cached_train_xlm-roberta-base_512.zip  \n",
      "  inflating: LBP/QA_EN_HI/dev-v2.0.json  \n",
      "  inflating: LBP/QA_EN_HI/train-v2.0.json  \n",
      "  inflating: LBP/run_squad.py        \n"
     ]
    }
   ],
   "source": [
    "! unzip LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1988e80-28fe-4686-86de-8a1b1e5b5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdl = [\"xlm-roberta\",151e-6,\"FacebookAI/xlm-roberta-base\",3]\n",
    "mdl = [\"xlm-roberta\",1e-4,\"FacebookAI/xlm-roberta-base\",4]\n",
    "\n",
    "# mdl = [\"bert\",1e-5,\"google/muril-base-cased\",3]\n",
    "# mdl = [\"indic\",151e-6,\"ai4bharat/indic-bert\",3]\n",
    "# mdl = [\"xlm-roberta\",1e-4,\"nozagleh/XLMr-ENIS-QA-Is-finetune-hi-course\",4]\n",
    "# mdl = [\"bert\",3e-3,\"google-bert/bert-large-uncased\",5]\n",
    "# mdl = [\"bert\",1e-5,\"google-bert/bert-base-multilingual-uncased\",5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810a1cc-85ea-470a-9d8b-6d62337d7e52",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fcb193e-313a-4e9e-8590-20b611823ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-19 04:50:37.873048: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-19 04:50:37.911630: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-19 04:50:37.911664: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-19 04:50:37.912696: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-19 04:50:37.918670: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "04/19/2024 04:50:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-base-multilingual-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/19/2024 04:50:40 - INFO - __main__ -   Training/evaluation parameters Namespace(model_type='bert', model_name_or_path='google-bert/bert-base-multilingual-uncased', output_dir='/tf/qa_en_hi/result/', data_dir='/tf/qa_en_hi/LBP/QA_EN_HI', train_file=None, predict_file=None, config_name='', tokenizer_name='', cache_dir='', version_2_with_negative=True, null_score_diff_threshold=0.0, max_seq_length=512, doc_stride=128, max_query_length=64, do_train=True, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=48, per_gpu_eval_batch_size=8, learning_rate=1e-05, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, n_best_size=20, max_answer_length=30, verbose_logging=False, lang_id=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, local_rank=-1, threads=1, n_gpu=1, device=device(type='cuda'))\n",
      "04/19/2024 04:50:40 - INFO - __main__ -   Loading features from cached file /tf/qa_en_hi/LBP/QA_EN_HI/cached_train_bert-base-multilingual-uncased_512\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "-----Learning Rate  1e-05\n",
      "04/19/2024 04:50:41 - INFO - __main__ -   ***** Running training *****\n",
      "Epoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:12,  1.81s/it]\u001b[A\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:02<00:08,  1.36s/it]\u001b[A\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:03<00:06,  1.22s/it]\u001b[A\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:04<00:04,  1.15s/it]\u001b[A\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:06<00:03,  1.12s/it]\u001b[A\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:07<00:02,  1.09s/it]\u001b[A\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:08<00:01,  1.08s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:08<00:00,  1.09s/it]\u001b[A\n",
      "Epoch: 100%|██████████████████████████████████████| 1/1 [00:08<00:00,  8.68s/it]\n",
      "04/19/2024 04:50:49 - INFO - __main__ -    global_step = 9, average loss = 5.3816657066345215\n",
      "04/19/2024 04:50:49 - INFO - __main__ -   Loading features from cached file /tf/qa_en_hi/LBP/QA_EN_HI/cached_dev_bert-base-multilingual-uncased_512\n",
      "04/19/2024 04:50:49 - INFO - __main__ -   ***** Running evaluation 9 *****\n",
      "04/19/2024 04:50:49 - INFO - __main__ -     Num examples = 100\n",
      "04/19/2024 04:50:49 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 17.02it/s]\n",
      "04/19/2024 04:50:50 - INFO - __main__ -     Evaluation done in total 0.764157 secs (0.007642 sec per example)\n",
      "=====\n",
      "PAVAN SAI\n",
      "\n",
      "OrderedDict([('exact', 5.555555555555555), ('f1', 17.273512312978514), ('total', 54), ('HasAns_exact', 5.555555555555555), ('HasAns_f1', 17.273512312978514), ('HasAns_total', 54), ('best_exact', 5.555555555555555), ('best_exact_thresh', 0.0), ('best_f1', 17.273512312978514), ('best_f1_thresh', 0.0)])\n",
      "=====\n",
      "\n",
      "04/19/2024 04:50:50 - INFO - __main__ -   Results: {}\n",
      "2024-04-19 04:50:55.066441: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-19 04:50:55.105213: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-19 04:50:55.105245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-19 04:50:55.106227: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-19 04:50:55.111988: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "04/19/2024 04:50:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-base-multilingual-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/19/2024 04:50:59 - INFO - __main__ -   Training/evaluation parameters Namespace(model_type='bert', model_name_or_path='google-bert/bert-base-multilingual-uncased', output_dir='/tf/qa_en_hi/result/', data_dir='/tf/qa_en_hi/LBP/QA_EN_HI', train_file=None, predict_file=None, config_name='', tokenizer_name='', cache_dir='', version_2_with_negative=True, null_score_diff_threshold=0.0, max_seq_length=512, doc_stride=128, max_query_length=64, do_train=True, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=48, per_gpu_eval_batch_size=8, learning_rate=2e-05, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, n_best_size=20, max_answer_length=30, verbose_logging=False, lang_id=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, local_rank=-1, threads=1, n_gpu=1, device=device(type='cuda'))\n",
      "04/19/2024 04:50:59 - INFO - __main__ -   Loading features from cached file /tf/qa_en_hi/LBP/QA_EN_HI/cached_train_bert-base-multilingual-uncased_512\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "-----Learning Rate  2e-05\n",
      "04/19/2024 04:50:59 - INFO - __main__ -   ***** Running training *****\n",
      "Epoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:12,  1.82s/it]\u001b[A\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:02<00:08,  1.37s/it]\u001b[A\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:03<00:06,  1.23s/it]\u001b[A\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:04<00:04,  1.16s/it]\u001b[A\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:06<00:03,  1.12s/it]\u001b[A\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:07<00:02,  1.10s/it]\u001b[A\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:08<00:01,  1.09s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:08<00:00,  1.09s/it]\u001b[A\n",
      "Epoch: 100%|██████████████████████████████████████| 1/1 [00:08<00:00,  8.73s/it]\n",
      "04/19/2024 04:51:08 - INFO - __main__ -    global_step = 9, average loss = 5.201043075985378\n",
      "04/19/2024 04:51:08 - INFO - __main__ -   Loading features from cached file /tf/qa_en_hi/LBP/QA_EN_HI/cached_dev_bert-base-multilingual-uncased_512\n",
      "04/19/2024 04:51:08 - INFO - __main__ -   ***** Running evaluation 9 *****\n",
      "04/19/2024 04:51:08 - INFO - __main__ -     Num examples = 100\n",
      "04/19/2024 04:51:08 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 16.89it/s]\n",
      "04/19/2024 04:51:08 - INFO - __main__ -     Evaluation done in total 0.770142 secs (0.007701 sec per example)\n",
      "=====\n",
      "PAVAN SAI\n",
      "\n",
      "OrderedDict([('exact', 7.407407407407407), ('f1', 14.177710609935673), ('total', 54), ('HasAns_exact', 7.407407407407407), ('HasAns_f1', 14.177710609935673), ('HasAns_total', 54), ('best_exact', 7.407407407407407), ('best_exact_thresh', 0.0), ('best_f1', 14.177710609935673), ('best_f1_thresh', 0.0)])\n",
      "=====\n",
      "\n",
      "04/19/2024 04:51:08 - INFO - __main__ -   Results: {}\n",
      "2024-04-19 04:51:13.265017: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-19 04:51:13.304112: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-19 04:51:13.304147: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-19 04:51:13.305187: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-19 04:51:13.311216: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "04/19/2024 04:51:15 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-base-multilingual-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/19/2024 04:51:17 - INFO - __main__ -   Training/evaluation parameters Namespace(model_type='bert', model_name_or_path='google-bert/bert-base-multilingual-uncased', output_dir='/tf/qa_en_hi/result/', data_dir='/tf/qa_en_hi/LBP/QA_EN_HI', train_file=None, predict_file=None, config_name='', tokenizer_name='', cache_dir='', version_2_with_negative=True, null_score_diff_threshold=0.0, max_seq_length=512, doc_stride=128, max_query_length=64, do_train=True, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=48, per_gpu_eval_batch_size=8, learning_rate=3e-05, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, n_best_size=20, max_answer_length=30, verbose_logging=False, lang_id=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, local_rank=-1, threads=1, n_gpu=1, device=device(type='cuda'))\n",
      "04/19/2024 04:51:17 - INFO - __main__ -   Loading features from cached file /tf/qa_en_hi/LBP/QA_EN_HI/cached_train_bert-base-multilingual-uncased_512\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "-----Learning Rate  3e-05\n",
      "04/19/2024 04:51:17 - INFO - __main__ -   ***** Running training *****\n",
      "Epoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:11,  1.63s/it]\u001b[A\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:02<00:07,  1.30s/it]\u001b[A\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:03<00:05,  1.19s/it]\u001b[A\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:04<00:04,  1.14s/it]\u001b[A\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:05<00:03,  1.11s/it]\u001b[A\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:06<00:02,  1.09s/it]\u001b[A\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:07<00:01,  1.08s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:08<00:00,  1.07s/it]\u001b[A\n",
      "Epoch: 100%|██████████████████████████████████████| 1/1 [00:08<00:00,  8.56s/it]\n",
      "04/19/2024 04:51:26 - INFO - __main__ -    global_step = 9, average loss = 5.025216738382976\n",
      "04/19/2024 04:51:26 - INFO - __main__ -   Loading features from cached file /tf/qa_en_hi/LBP/QA_EN_HI/cached_dev_bert-base-multilingual-uncased_512\n",
      "04/19/2024 04:51:26 - INFO - __main__ -   ***** Running evaluation 9 *****\n",
      "04/19/2024 04:51:26 - INFO - __main__ -     Num examples = 100\n",
      "04/19/2024 04:51:26 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 16.85it/s]\n",
      "04/19/2024 04:51:27 - INFO - __main__ -     Evaluation done in total 0.771674 secs (0.007717 sec per example)\n",
      "=====\n",
      "PAVAN SAI\n",
      "\n",
      "OrderedDict([('exact', 7.407407407407407), ('f1', 11.101664837474951), ('total', 54), ('HasAns_exact', 7.407407407407407), ('HasAns_f1', 11.101664837474951), ('HasAns_total', 54), ('best_exact', 7.407407407407407), ('best_exact_thresh', 0.0), ('best_f1', 11.101664837474951), ('best_f1_thresh', 0.0)])\n",
      "=====\n",
      "\n",
      "04/19/2024 04:51:27 - INFO - __main__ -   Results: {}\n",
      "2024-04-19 04:51:31.496070: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-19 04:51:31.535318: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-19 04:51:31.535354: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-19 04:51:31.536376: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-19 04:51:31.542320: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "04/19/2024 04:51:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-base-multilingual-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/19/2024 04:51:35 - INFO - __main__ -   Training/evaluation parameters Namespace(model_type='bert', model_name_or_path='google-bert/bert-base-multilingual-uncased', output_dir='/tf/qa_en_hi/result/', data_dir='/tf/qa_en_hi/LBP/QA_EN_HI', train_file=None, predict_file=None, config_name='', tokenizer_name='', cache_dir='', version_2_with_negative=True, null_score_diff_threshold=0.0, max_seq_length=512, doc_stride=128, max_query_length=64, do_train=True, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=48, per_gpu_eval_batch_size=8, learning_rate=4e-05, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, n_best_size=20, max_answer_length=30, verbose_logging=False, lang_id=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, local_rank=-1, threads=1, n_gpu=1, device=device(type='cuda'))\n",
      "04/19/2024 04:51:35 - INFO - __main__ -   Loading features from cached file /tf/qa_en_hi/LBP/QA_EN_HI/cached_train_bert-base-multilingual-uncased_512\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "-----Learning Rate  4e-05\n",
      "04/19/2024 04:51:35 - INFO - __main__ -   ***** Running training *****\n",
      "Epoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:10,  1.54s/it]\u001b[A\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:02<00:07,  1.26s/it]\u001b[A\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:03<00:05,  1.17s/it]\u001b[A\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:04<00:04,  1.13s/it]\u001b[A\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:05<00:03,  1.10s/it]\u001b[A\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:06<00:02,  1.09s/it]\u001b[A\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:07<00:01,  1.08s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:08<00:00,  1.06s/it]\u001b[A\n",
      "Epoch: 100%|██████████████████████████████████████| 1/1 [00:08<00:00,  8.49s/it]\n",
      "04/19/2024 04:51:43 - INFO - __main__ -    global_step = 9, average loss = 4.85254086388482\n",
      "04/19/2024 04:51:43 - INFO - __main__ -   Loading features from cached file /tf/qa_en_hi/LBP/QA_EN_HI/cached_dev_bert-base-multilingual-uncased_512\n",
      "04/19/2024 04:51:43 - INFO - __main__ -   ***** Running evaluation 9 *****\n",
      "04/19/2024 04:51:43 - INFO - __main__ -     Num examples = 100\n",
      "04/19/2024 04:51:43 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 16.83it/s]\n",
      "04/19/2024 04:51:44 - INFO - __main__ -     Evaluation done in total 0.772832 secs (0.007728 sec per example)\n",
      "=====\n",
      "PAVAN SAI\n",
      "\n",
      "OrderedDict([('exact', 1.8518518518518519), ('f1', 6.424312415024489), ('total', 54), ('HasAns_exact', 1.8518518518518519), ('HasAns_f1', 6.424312415024489), ('HasAns_total', 54), ('best_exact', 1.8518518518518519), ('best_exact_thresh', 0.0), ('best_f1', 6.424312415024489), ('best_f1_thresh', 0.0)])\n",
      "=====\n",
      "\n",
      "04/19/2024 04:51:44 - INFO - __main__ -   Results: {}\n",
      "2024-04-19 04:51:48.864496: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-19 04:51:48.903488: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-19 04:51:48.903522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-19 04:51:48.904560: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-19 04:51:48.910508: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "04/19/2024 04:51:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-base-multilingual-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/19/2024 04:51:52 - INFO - __main__ -   Training/evaluation parameters Namespace(model_type='bert', model_name_or_path='google-bert/bert-base-multilingual-uncased', output_dir='/tf/qa_en_hi/result/', data_dir='/tf/qa_en_hi/LBP/QA_EN_HI', train_file=None, predict_file=None, config_name='', tokenizer_name='', cache_dir='', version_2_with_negative=True, null_score_diff_threshold=0.0, max_seq_length=512, doc_stride=128, max_query_length=64, do_train=True, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=48, per_gpu_eval_batch_size=8, learning_rate=5e-05, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, n_best_size=20, max_answer_length=30, verbose_logging=False, lang_id=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, local_rank=-1, threads=1, n_gpu=1, device=device(type='cuda'))\n",
      "04/19/2024 04:51:52 - INFO - __main__ -   Loading features from cached file /tf/qa_en_hi/LBP/QA_EN_HI/cached_train_bert-base-multilingual-uncased_512\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "-----Learning Rate  5e-05\n",
      "04/19/2024 04:51:52 - INFO - __main__ -   ***** Running training *****\n",
      "Epoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:11,  1.66s/it]\u001b[A\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:02<00:07,  1.31s/it]\u001b[A\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:03<00:05,  1.20s/it]\u001b[A\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:04<00:04,  1.14s/it]\u001b[A\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:05<00:03,  1.12s/it]\u001b[A\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:06<00:02,  1.10s/it]\u001b[A\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:08<00:01,  1.09s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:08<00:00,  1.08s/it]\u001b[A\n",
      "Epoch: 100%|██████████████████████████████████████| 1/1 [00:08<00:00,  8.62s/it]\n",
      "04/19/2024 04:52:01 - INFO - __main__ -    global_step = 9, average loss = 4.696835888756646\n",
      "04/19/2024 04:52:01 - INFO - __main__ -   Loading features from cached file /tf/qa_en_hi/LBP/QA_EN_HI/cached_dev_bert-base-multilingual-uncased_512\n",
      "04/19/2024 04:52:01 - INFO - __main__ -   ***** Running evaluation 9 *****\n",
      "04/19/2024 04:52:01 - INFO - __main__ -     Num examples = 100\n",
      "04/19/2024 04:52:01 - INFO - __main__ -     Batch size = 8\n",
      "Evaluating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 16.84it/s]\n",
      "04/19/2024 04:52:02 - INFO - __main__ -     Evaluation done in total 0.772404 secs (0.007724 sec per example)\n",
      "=====\n",
      "PAVAN SAI\n",
      "\n",
      "OrderedDict([('exact', 1.8518518518518519), ('f1', 6.6916519694297465), ('total', 54), ('HasAns_exact', 1.8518518518518519), ('HasAns_f1', 6.6916519694297465), ('HasAns_total', 54), ('best_exact', 1.8518518518518519), ('best_exact_thresh', 0.0), ('best_f1', 6.6916519694297465), ('best_f1_thresh', 0.0)])\n",
      "=====\n",
      "\n",
      "04/19/2024 04:52:02 - INFO - __main__ -   Results: {}\n"
     ]
    }
   ],
   "source": [
    "alpha=[1e-5,2e-5,3e-5,4e-5,5e-5]\n",
    "for dis in alpha:\n",
    "    ! python /tf/qa_en_hi/LBP/run_squad.py --learning_rate {dis}  --data_dir /tf/qa_en_hi/LBP/QA_EN_HI --output_dir /tf/qa_en_hi/result/  --model_type {mdl[0]}  --model_name_or_path  {mdl[2]} --do_train   --do_eval   --version_2_with_negative   --num_train_epochs 1   --per_gpu_train_batch_size 48   --max_seq_length 512   --overwrite_output_dir   --save_steps -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc470b3-2a17-4dda-92f9-53e414603d93",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82260629-5a23-4cea-8592-d6eb3f2ed69a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-19 04:55:10.495077: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-19 04:55:10.533842: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-19 04:55:10.533877: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-19 04:55:10.534912: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-19 04:55:10.540950: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "04/19/2024 04:55:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n",
      "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/19/2024 04:55:16 - INFO - __main__ -   Training/evaluation parameters Namespace(model_type='xlm-roberta', model_name_or_path='FacebookAI/xlm-roberta-base', output_dir='/tf/qa_en_hi/result/', data_dir='/tf/qa_en_hi/LBP/QA_EN_HI', train_file=None, predict_file=None, config_name='', tokenizer_name='', cache_dir='', version_2_with_negative=True, null_score_diff_threshold=0.0, max_seq_length=512, doc_stride=128, max_query_length=64, do_train=True, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=48, per_gpu_eval_batch_size=48, learning_rate=0.0001, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=4.0, max_steps=-1, warmup_steps=0, n_best_size=20, max_answer_length=30, verbose_logging=False, lang_id=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, local_rank=-1, threads=1, n_gpu=1, device=device(type='cuda'))\n",
      "04/19/2024 04:55:16 - INFO - __main__ -   Loading features from cached file /tf/qa_en_hi/LBP/QA_EN_HI/cached_train_xlm-roberta-base_512\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "-----Learning Rate  0.0001\n",
      "04/19/2024 04:55:16 - INFO - __main__ -   ***** Running training *****\n",
      "Epoch:   0%|                                              | 0/4 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:11,  1.70s/it]\u001b[A\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:02<00:08,  1.34s/it]\u001b[A\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:03<00:06,  1.22s/it]\u001b[A\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:04<00:04,  1.16s/it]\u001b[A\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:06<00:03,  1.13s/it]\u001b[A\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:07<00:02,  1.12s/it]\u001b[A\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:08<00:01,  1.11s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:08<00:00,  1.12s/it]\u001b[A\n",
      "Epoch:  25%|█████████▌                            | 1/4 [00:08<00:26,  8.92s/it]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:07,  1.08s/it]\u001b[A\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:02<00:06,  1.08s/it]\u001b[A\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:03<00:05,  1.08s/it]\u001b[A\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:04<00:04,  1.09s/it]\u001b[A\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:05<00:03,  1.09s/it]\u001b[A\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:06<00:02,  1.09s/it]\u001b[A\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:07<00:01,  1.09s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:08<00:00,  1.04s/it]\u001b[A\n",
      "Epoch:  50%|███████████████████                   | 2/4 [00:17<00:17,  8.58s/it]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:07,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:02<00:06,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:03<00:05,  1.09s/it]\u001b[A\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:04<00:04,  1.09s/it]\u001b[A\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:05<00:03,  1.09s/it]\u001b[A\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:06<00:02,  1.09s/it]\u001b[A\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:07<00:01,  1.09s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:08<00:00,  1.05s/it]\u001b[A\n",
      "Epoch:  75%|████████████████████████████▌         | 3/4 [00:25<00:08,  8.49s/it]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:07,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:02<00:06,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:03<00:05,  1.09s/it]\u001b[A\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:04<00:04,  1.09s/it]\u001b[A\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:05<00:03,  1.09s/it]\u001b[A\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:06<00:02,  1.09s/it]\u001b[A\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:07<00:01,  1.09s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:08<00:00,  1.05s/it]\u001b[A\n",
      "Epoch: 100%|██████████████████████████████████████| 4/4 [00:34<00:00,  8.51s/it]\n",
      "04/19/2024 04:55:50 - INFO - __main__ -    global_step = 33, average loss = 3.6571000995058003\n",
      "04/19/2024 04:55:50 - INFO - __main__ -   Loading features from cached file /tf/qa_en_hi/LBP/QA_EN_HI/cached_dev_xlm-roberta-base_512\n",
      "04/19/2024 04:55:51 - INFO - __main__ -   ***** Running evaluation 33 *****\n",
      "04/19/2024 04:55:51 - INFO - __main__ -     Num examples = 101\n",
      "04/19/2024 04:55:51 - INFO - __main__ -     Batch size = 48\n",
      "Evaluating: 100%|█████████████████████████████████| 3/3 [00:00<00:00,  3.99it/s]\n",
      "04/19/2024 04:55:51 - INFO - __main__ -     Evaluation done in total 0.752203 secs (0.007448 sec per example)\n",
      "=====\n",
      "PAVAN SAI\n",
      "\n",
      "OrderedDict([('exact', 22.22222222222222), ('f1', 26.929012345679013), ('total', 54), ('HasAns_exact', 22.22222222222222), ('HasAns_f1', 26.929012345679013), ('HasAns_total', 54), ('best_exact', 22.22222222222222), ('best_exact_thresh', 0.0), ('best_f1', 26.929012345679013), ('best_f1_thresh', 0.0)])\n",
      "=====\n",
      "\n",
      "04/19/2024 04:55:51 - INFO - __main__ -   Results: {}\n"
     ]
    }
   ],
   "source": [
    "! python /tf/qa_en_hi/LBP/run_squad.py --learning_rate {mdl[1]} --output_dir /tf/qa_en_hi/result/  --data_dir /tf/qa_en_hi/LBP/QA_EN_HI --model_type {mdl[0]} --learning_rate {mdl[1]} --model_name_or_path  {mdl[2]} --do_train   --do_eval   --version_2_with_negative   --num_train_epochs {mdl[3]}   --per_gpu_train_batch_size 48 --per_gpu_eval_batch_size 48  --max_seq_length 512   --overwrite_output_dir   --save_steps -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6fb407-180b-4cef-98dd-96ece253490b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86f5ea-6728-4fd8-8e92-f1b4c5474d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python /tf/qa_en_hi/qa.py --output_dir /tf/qa_en_hi/result/  --train_data /tf/qa_en_hi/LBP/QA_EN_HI/train-v2.0.json --eval_data /tf/qa_en_hi/LBP/QA_EN_HI/dev-v2.0.json --learning_rate {mdl[1]} --model_name  {mdl[2]} --do_train   --do_predict  --train_batch_size 16   --num_train_epochs {mdl[3]}    --max_length 512   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04184f08-eade-42ed-aa83-34ec26d1a933",
   "metadata": {},
   "source": [
    "# MODEL TRAINING ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983218b8-dbbd-4ff7-b2e3-9d4bec7dbb9e",
   "metadata": {},
   "source": [
    "--train_file /tf/qa_en_hi/LBP/QA_EN_HI/train-v2.0.json --predict_file /tf/qa_en_hi/LBP/QA_EN_HI/train-v2.0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4af6fcea-9426-4f97-a8fa-041c04b2523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-19 04:55:56.245589: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-19 04:55:56.284328: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-19 04:55:56.284361: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-19 04:55:56.285335: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-19 04:55:56.290974: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "04/19/2024 04:55:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n",
      "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/19/2024 04:56:01 - INFO - __main__ -   Training/evaluation parameters Namespace(model_type='xlm-roberta', model_name_or_path='FacebookAI/xlm-roberta-base', output_dir='/tf/qa_en_hi/result/', data_dir=None, train_file='/tf/qa_en_hi/LBP/QA_EN_HI/train-v2.0.json', predict_file='/tf/qa_en_hi/LBP/QA_EN_HI/train-v2.0.json', config_name='', tokenizer_name='', cache_dir='', version_2_with_negative=True, null_score_diff_threshold=0.0, max_seq_length=512, doc_stride=128, max_query_length=64, do_train=True, do_eval=True, evaluate_during_training=False, do_lower_case=False, per_gpu_train_batch_size=48, per_gpu_eval_batch_size=48, learning_rate=0.0001, gradient_accumulation_steps=1, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=4.0, max_steps=-1, warmup_steps=0, n_best_size=20, max_answer_length=30, verbose_logging=False, lang_id=0, logging_steps=500, save_steps=-1, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, local_rank=-1, threads=1, n_gpu=1, device=device(type='cuda'))\n",
      "04/19/2024 04:56:01 - INFO - __main__ -   Loading features from cached file ./cached_train_xlm-roberta-base_512\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "-----Learning Rate  0.0001\n",
      "04/19/2024 04:56:01 - INFO - __main__ -   ***** Running training *****\n",
      "Epoch:   0%|                                              | 0/4 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:11,  1.68s/it]\u001b[A\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:02<00:08,  1.33s/it]\u001b[A\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:03<00:06,  1.22s/it]\u001b[A\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:04<00:04,  1.17s/it]\u001b[A\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:06<00:03,  1.14s/it]\u001b[A\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:07<00:02,  1.13s/it]\u001b[A\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:08<00:01,  1.12s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:08<00:00,  1.12s/it]\u001b[A\n",
      "Epoch:  25%|█████████▌                            | 1/4 [00:08<00:26,  8.99s/it]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:07,  1.10s/it]\u001b[A\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:02<00:06,  1.10s/it]\u001b[A\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:03<00:05,  1.10s/it]\u001b[A\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:04<00:04,  1.10s/it]\u001b[A\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:05<00:03,  1.10s/it]\u001b[A\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:06<00:02,  1.10s/it]\u001b[A\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:07<00:01,  1.10s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:08<00:00,  1.05s/it]\u001b[A\n",
      "Epoch:  50%|███████████████████                   | 2/4 [00:17<00:17,  8.66s/it]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:07,  1.10s/it]\u001b[A\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:02<00:06,  1.10s/it]\u001b[A\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:03<00:05,  1.10s/it]\u001b[A\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:04<00:04,  1.10s/it]\u001b[A\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:05<00:03,  1.10s/it]\u001b[A\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:06<00:02,  1.10s/it]\u001b[A\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:07<00:01,  1.10s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:08<00:00,  1.06s/it]\u001b[A\n",
      "Epoch:  75%|████████████████████████████▌         | 3/4 [00:25<00:08,  8.56s/it]\n",
      "Iteration:   0%|                                          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  12%|████▎                             | 1/8 [00:01<00:07,  1.10s/it]\u001b[A\n",
      "Iteration:  25%|████████▌                         | 2/8 [00:02<00:06,  1.10s/it]\u001b[A\n",
      "Iteration:  38%|████████████▊                     | 3/8 [00:03<00:05,  1.10s/it]\u001b[A\n",
      "Iteration:  50%|█████████████████                 | 4/8 [00:04<00:04,  1.10s/it]\u001b[A\n",
      "Iteration:  62%|█████████████████████▎            | 5/8 [00:05<00:03,  1.10s/it]\u001b[A\n",
      "Iteration:  75%|█████████████████████████▌        | 6/8 [00:06<00:02,  1.10s/it]\u001b[A\n",
      "Iteration:  88%|█████████████████████████████▊    | 7/8 [00:07<00:01,  1.10s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████████| 8/8 [00:08<00:00,  1.06s/it]\u001b[A\n",
      "Epoch: 100%|██████████████████████████████████████| 4/4 [00:34<00:00,  8.58s/it]\n",
      "04/19/2024 04:56:35 - INFO - __main__ -    global_step = 33, average loss = 3.6571000995058003\n",
      "04/19/2024 04:56:35 - INFO - __main__ -   Loading features from cached file ./cached_dev_xlm-roberta-base_512\n",
      "04/19/2024 04:56:35 - INFO - __main__ -   ***** Running evaluation 33 *****\n",
      "04/19/2024 04:56:35 - INFO - __main__ -     Num examples = 368\n",
      "04/19/2024 04:56:35 - INFO - __main__ -     Batch size = 48\n",
      "Evaluating: 100%|█████████████████████████████████| 8/8 [00:02<00:00,  2.92it/s]\n",
      "04/19/2024 04:56:38 - INFO - __main__ -     Evaluation done in total 2.736280 secs (0.007436 sec per example)\n",
      "=====\n",
      "PAVAN SAI\n",
      "\n",
      "OrderedDict([('exact', 39.38223938223938), ('f1', 42.94405313312878), ('total', 259), ('HasAns_exact', 39.38223938223938), ('HasAns_f1', 42.94405313312878), ('HasAns_total', 259), ('best_exact', 39.38223938223938), ('best_exact_thresh', 0.0), ('best_f1', 42.94405313312878), ('best_f1_thresh', 0.0)])\n",
      "=====\n",
      "\n",
      "04/19/2024 04:56:39 - INFO - __main__ -   Results: {}\n"
     ]
    }
   ],
   "source": [
    "! python /tf/qa_en_hi/LBP/run_squad.py   --output_dir /tf/qa_en_hi/result/  --train_file /tf/qa_en_hi/LBP/QA_EN_HI/train-v2.0.json --predict_file /tf/qa_en_hi/LBP/QA_EN_HI/train-v2.0.json --learning_rate {mdl[1]} --model_type {mdl[0]} --model_name  {mdl[2]} --do_train   --do_eval   --version_2_with_negative   --num_train_epochs {mdl[3]}   --per_gpu_train_batch_size 48 --per_gpu_eval_batch_size 48   --max_seq_length 512   --overwrite_output_dir   --save_steps -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455ad55-979f-434f-9089-73c5684f8e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
